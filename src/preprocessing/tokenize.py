# src/preprocessing/tokenize.py
# Small helper tokenizer used in other modules if needed.

def tokenize(text: str) -> list[str]:
    return text.split()
